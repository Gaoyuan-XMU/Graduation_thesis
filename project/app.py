import gradio as gr
import os
import subprocess
import glob
from trans import transcribe_audio
from request import query_dify
from speak import text_to_speech

CONFIG = {
    "transcript_path": os.path.join(os.getcwd(), "output", "transcript.txt"),
    "answer_path": os.path.join(os.getcwd(), "output", "answer.txt"),
    "speech_path": os.path.join(os.getcwd(), "output", "answer.mp3"),
    "api_key": "app-gzynEHxAF8Djn8QkWwNMRtcC"
}

SADTALKER_PATH = "G:/Last/AI_Digital_Human"  # âš ï¸ æ›¿æ¢ä¸ºä½ æœ¬åœ°å®é™…è·¯å¾„
os.makedirs("output", exist_ok=True)

def validate_file(path):
    return bool(path and os.path.exists(path) and os.path.getsize(path) > 0)

def generate_video_with_sadtalker(image_path, audio_path, output_dir):
    venv_python = os.path.join(SADTALKER_PATH, "venv", "Scripts", "python.exe")
    inference_script = os.path.join(SADTALKER_PATH, "inference.py")

    if not os.path.exists(venv_python):
        raise RuntimeError(f"è™šæ‹Ÿç¯å¢ƒæœªæ‰¾åˆ°: {venv_python}")

    cmd = [
        venv_python,
        inference_script,
        "--driven_audio", audio_path,
        "--source_image", image_path,
        "--result_dir", output_dir
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        raise RuntimeError(f"Sadtalker æ‰§è¡Œå¤±è´¥: {result.stderr}")

    return find_latest_mp4(output_dir)

def find_latest_mp4(output_dir):
    mp4_files = glob.glob(os.path.join(output_dir, "**", "*.mp4"), recursive=True)
    if not mp4_files:
        raise RuntimeError("æœªæ‰¾åˆ°ä»»ä½• .mp4 æ–‡ä»¶")
    return max(mp4_files, key=os.path.getctime)

def process_pipeline(history, audio_path, source_image):
    try:
        if not validate_file(audio_path):
            raise RuntimeError("æœªæ”¶åˆ°æœ‰æ•ˆçš„éŸ³é¢‘æ–‡ä»¶")

        transcript = transcribe_audio(audio_path, CONFIG["transcript_path"])
        if not validate_file(CONFIG["transcript_path"]):
            raise RuntimeError("è½¬å†™æ–‡ä»¶ç”Ÿæˆå¤±è´¥")

        answer = query_dify(CONFIG["transcript_path"], CONFIG["answer_path"], CONFIG["api_key"])
        if not validate_file(CONFIG["answer_path"]):
            raise RuntimeError("APIå“åº”æ–‡ä»¶ç”Ÿæˆå¤±è´¥")

        text_to_speech(CONFIG["answer_path"], CONFIG["speech_path"])
        if not validate_file(CONFIG["speech_path"]):
            raise RuntimeError("è¯­éŸ³æ–‡ä»¶ç”Ÿæˆå¤±è´¥")

        if not source_image or not os.path.exists(source_image):
            raise RuntimeError("è¯·ä¸Šä¼ æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶")

        output_dir = os.path.join(os.getcwd(), "output")
        video_path = generate_video_with_sadtalker(source_image, CONFIG["speech_path"], output_dir)

        new_history = history + [(transcript, answer)]
        return new_history, video_path, ""
    except Exception as e:
        return history, None, f"âŒ é”™è¯¯: {str(e)}"

def render_autoplay_video(history, video_path, error):
    if video_path:
        html = f"""
        <video width='100%' autoplay playsinline controls>
            <source src='file/{video_path}' type='video/mp4'>
            æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒ video æ ‡ç­¾ã€‚
        </video>
        """
        return history, html, error
    return history, "", error

def reset_conversation():
    try:
        if os.path.exists("output/conversation_id.txt"):
            os.remove("output/conversation_id.txt")
        return [], "", "å¯¹è¯å·²é‡ç½®"
    except Exception as e:
        return [], "", f"é‡ç½®å¤±è´¥: {str(e)}"

with gr.Blocks(title="æ•°å­—äººåœ¨çº¿äº¤äº’å¹³å°", theme="soft") as demo:
    gr.Markdown("""
    # ğŸ¤– æ•°å­—äººåœ¨çº¿äº¤äº’å¹³å°
    æ¬¢è¿æ¥åˆ°å®æ—¶è¯­éŸ³å¯¹è¯ç³»ç»Ÿï¼Œä¸Šä¼ äººç‰©å›¾åƒå¹¶ä¸æ•°å­—äººå¯¹è¯ã€‚
    """)

    with gr.Row():
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ’¬ å¯¹è¯å†å²")
            history = gr.Chatbot(label="", height=400).style(container=True, height=400)
            error_output = gr.Markdown(value="", elem_id="status_text", visible=True)

        with gr.Column(scale=1):
            gr.Markdown("### ğŸ–¼ï¸ äººç‰©å›¾åƒä¸Šä¼ ")
            source_image_input = gr.Image(label="ä¸Šä¼ å¤´åƒç…§ç‰‡ç”¨äºé©±åŠ¨ï¼ˆæ”¯æŒ JPG/PNGï¼‰", type="filepath")

            gr.Markdown("### ğŸ™ï¸ è¯­éŸ³è¾“å…¥")
            audio_input = gr.Audio(source="microphone", type="filepath", label="ç‚¹å‡»å½•éŸ³")

            submit_btn = gr.Button("ğŸ“¤ æäº¤è¯­éŸ³", variant="primary")
            reset_btn = gr.Button("ğŸ”„ é‡ç½®å¯¹è¯", variant="secondary")

            gr.Markdown("### ğŸ“½ï¸ åˆæˆè§†é¢‘é¢„è§ˆ")
            video_output = gr.HTML(label="åˆæˆè§†é¢‘")

    video_path_state = gr.State()

    submit_btn.click(
        fn=process_pipeline,
        inputs=[history, audio_input, source_image_input],
        outputs=[history, video_path_state, error_output]
    ).then(
        fn=render_autoplay_video,
        inputs=[history, video_path_state, error_output],
        outputs=[history, video_output, error_output]
    )

    reset_btn.click(
        fn=reset_conversation,
        outputs=[history, video_output, error_output]
    )

if __name__ == "__main__":
    demo.launch(server_port=7860, show_error=True, enable_queue=True)
